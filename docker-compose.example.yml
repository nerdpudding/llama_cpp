# =============================================================================
# llama.cpp server -- RTX 4090 + RTX 5070 Ti
#
# HOW TO USE:
#
# 1. Run the model selector (recommended):
#      ./start.sh                    # Interactive menu + monitoring dashboard
#      ./start.sh qwen3-coder        # Direct launch by model ID
#      ./start.sh --no-dashboard     # Launch without dashboard (raw logs)
#
#    start.sh reads models.conf, generates .env, starts the container, and
#    opens a curses dashboard with server logs, GPU/system monitoring, and
#    keyboard controls ([q] stop & exit, [r] stop & return to menu).
#
# 2. Or manually:
#      Edit .env (or copy from .env.example) and run: docker compose up
#
# 3. Access:
#      Web UI:  http://localhost:8080
#      API:     http://localhost:8080/v1/chat/completions
#
# MODELS:
#   All models are defined in models.conf. Run ./start.sh --list to see them.
#
# VARIABLE REFERENCE:
#
#   MODEL           Path to .gguf file relative to models/ dir
#   CTX_SIZE        Context window size in tokens
#   N_GPU_LAYERS    Number of layers to offload to GPU (99 = all)
#   SPLIT_MODE      Multi-GPU split: layer (default), row, none
#   TENSOR_SPLIT    Manual split ratio (blank = auto)
#   MAIN_GPU        Primary GPU index (0 = RTX 4090)
#   FLASH_ATTN      Flash attention (1 = on)
#   KV_CACHE_TYPE_K KV cache key type (q8_0 recommended)
#   KV_CACHE_TYPE_V KV cache value type (q8_0 recommended)
#   FIT             Auto-fit VRAM (on/off, use off with -ot)
#   FIT_TARGET      VRAM headroom in MiB per device
#   FIT_CTX         Minimum context size for auto-fit
#   EXTRA_ARGS      Any additional llama-server flags
#
# =============================================================================

services:
  llama-server:
    build: .
    container_name: llama-server
    ports:
      - "8080:8080"
    volumes:
      - ./models:/models:ro
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    environment:
      # ---- Model ----
      - MODEL=/models/${MODEL:-model.gguf}
      # ---- Context ----
      - CTX_SIZE=${CTX_SIZE:-131072}
      # ---- GPU offload ----
      - N_GPU_LAYERS=${N_GPU_LAYERS:-99}
      # ---- Split mode: layer (default), row, none ----
      - SPLIT_MODE=${SPLIT_MODE:-layer}
      # ---- Manual tensor split ratio (blank = use --fit auto) ----
      - TENSOR_SPLIT=${TENSOR_SPLIT:-}
      # ---- Main GPU (0 = RTX 4090, 1 = RTX 5070 Ti) ----
      - MAIN_GPU=${MAIN_GPU:-0}
      # ---- Flash attention ----
      - FLASH_ATTN=${FLASH_ATTN:-1}
      # ---- KV cache quantization ----
      - KV_CACHE_TYPE_K=${KV_CACHE_TYPE_K:-q8_0}
      - KV_CACHE_TYPE_V=${KV_CACHE_TYPE_V:-q8_0}
      # ---- Auto-fit (on by default, adjusts layers/ctx to fit VRAM) ----
      - FIT=${FIT:-on}
      # ---- Fit target: VRAM headroom in MiB per device (e.g. 1024 or 512,4096) ----
      - FIT_TARGET=${FIT_TARGET:-}
      # ---- Fit minimum context size ----
      - FIT_CTX=${FIT_CTX:-}
      # ---- Extra flags (pass anything not covered above) ----
      - EXTRA_ARGS=${EXTRA_ARGS:-}
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:8080/health"]
      interval: 10s
      timeout: 5s
      retries: 60
      start_period: 120s
    restart: unless-stopped

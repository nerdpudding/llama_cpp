#!/bin/bash
# Claude Code with local llama-server backend
# Requires: llama-server running on localhost:8080
#           bubblewrap + socat installed (for /sandbox)
#
# Source of truth: <repo>/claude-local/bin/claude-local
# Installed to: ~/bin/claude-local
# See claude-local/README.md for setup instructions.

# Pre-flight check: is llama-server running?
if ! curl -s -o /dev/null --connect-timeout 2 http://127.0.0.1:8080/health; then
    echo "Error: llama-server is not reachable at http://127.0.0.1:8080"
    echo ""
    echo "Start a model first with ./start.sh from the llama_cpp project root."
    echo "Then try again."
    exit 1
fi

# Use CLAUDE_CONFIG_DIR instead of HOME override â€” avoids side effects
# (missing binary warnings, broken path resolution) while still isolating
# Claude Code config (settings, CLAUDE.md, skills) from ~/.claude/.
export CLAUDE_CONFIG_DIR="$HOME/.claude-local"

export ANTHROPIC_BASE_URL=http://127.0.0.1:8080
export ANTHROPIC_AUTH_TOKEN=llamacpp
export ANTHROPIC_API_KEY=""
export ANTHROPIC_MODEL=local-llama
export ANTHROPIC_SMALL_FAST_MODEL=local-llama
export CLAUDE_CODE_DISABLE_NONESSENTIAL_TRAFFIC=1

# Pass through any arguments (e.g. --model qwen3-coder-ud-q5)
exec claude "$@"

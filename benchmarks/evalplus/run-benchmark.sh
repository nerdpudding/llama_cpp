#!/usr/bin/env bash
# =============================================================================
# run-benchmark.sh — EvalPlus HumanEval+ benchmark runner
#
# Runs bench-* profiles from models.conf sequentially, generating code via
# the llama.cpp OpenAI-compatible API and evaluating in a Docker sandbox.
#
# Usage:
#   ./run-benchmark.sh                        # All bench-* models
#   ./run-benchmark.sh bench-qwen3-coder      # Specific model(s)
#   ./run-benchmark.sh --list                  # List benchmark profiles
#   ./run-benchmark.sh --report               # Generate comparison report
# =============================================================================

set -euo pipefail

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_DIR="$(dirname "$(dirname "$SCRIPT_DIR")")"
CONF="$PROJECT_DIR/models.conf"
MODELS_DIR="$PROJECT_DIR/models"
ENV_FILE="$PROJECT_DIR/.env"
COMPOSE_FILE="$PROJECT_DIR/docker-compose.yml"
RESULTS_DIR="$SCRIPT_DIR/results"
VENV_DIR="$SCRIPT_DIR/.venv"
REPORT_SCRIPT="$SCRIPT_DIR/generate-report.py"
REFERENCE_FILE="$SCRIPT_DIR/reference-scores.json"

HEALTH_TIMEOUT=600  # 10 minutes — large models need time to load
HEALTH_URL="http://localhost:8080/health"
EVALPLUS_DOCKER="ganler/evalplus:latest"

# --- INI parser (from start.sh) ---------------------------------------------

declare -a SECTION_IDS=()
declare -a SECTION_NAMES=()
declare -A CONFIG=()

parse_conf() {
    local section=""
    while IFS= read -r line || [[ -n "$line" ]]; do
        line="${line#"${line%%[![:space:]]*}"}"
        line="${line%"${line##*[![:space:]]}"}"
        [[ -z "$line" || "$line" == \#* ]] && continue
        if [[ "$line" =~ ^\[([a-zA-Z0-9_-]+)\]$ ]]; then
            section="${BASH_REMATCH[1]}"
            SECTION_IDS+=("$section")
            continue
        fi
        if [[ -n "$section" && "$line" == *=* ]]; then
            local key="${line%%=*}"
            local value="${line#*=}"
            CONFIG["${section}.${key}"]="$value"
            if [[ "$key" == "NAME" ]]; then
                SECTION_NAMES+=("$value")
            fi
        fi
    done < "$CONF"
}

get() { echo "${CONFIG["${1}.${2}"]:-}"; }

die() { echo "ERROR: $*" >&2; exit 1; }

log() { echo "[$(date '+%H:%M:%S')] $*"; }

# --- Helpers -----------------------------------------------------------------

get_bench_ids() {
    local ids=()
    for id in "${SECTION_IDS[@]}"; do
        [[ "$id" == bench-* ]] && ids+=("$id")
    done
    echo "${ids[@]}"
}

list_bench_models() {
    echo "Available benchmark profiles (edit models.conf to change settings):"
    echo ""
    for id in "${SECTION_IDS[@]}"; do
        [[ "$id" != bench-* ]] && continue
        local name; name=$(get "$id" NAME)
        local model; model=$(get "$id" MODEL)
        local ctx; ctx=$(get "$id" CTX_SIZE)
        local ngl; ngl=$(get "$id" N_GPU_LAYERS)
        local fit; fit=$(get "$id" FIT)
        local extra; extra=$(get "$id" EXTRA_ARGS)
        echo "  [$id]"
        echo "    Name:       $name"
        echo "    Model:      $model"
        echo "    Context:    $ctx"
        echo "    GPU layers: $ngl"
        [[ -n "$fit" ]] && echo "    Fit:        $fit"
        [[ -n "$extra" ]] && echo "    Extra args: $extra"
        echo ""
    done
}

# --- Prerequisite checks ----------------------------------------------------

check_prerequisites() {
    if ! command -v docker &>/dev/null; then
        die "docker not found. Install Docker first."
    fi
    if ! docker info &>/dev/null 2>&1; then
        die "Docker daemon is not running."
    fi
    if [[ ! -d "$VENV_DIR" ]]; then
        die "Python venv not found at $VENV_DIR
Run setup first:
  cd $SCRIPT_DIR
  uv venv
  source .venv/bin/activate
  uv pip install evalplus"
    fi
    if ! "$VENV_DIR/bin/python" -c "import evalplus" 2>/dev/null; then
        die "evalplus not installed in $VENV_DIR
Run: source $VENV_DIR/bin/activate && uv pip install evalplus"
    fi
}

check_no_running_container() {
    local state
    state=$(docker inspect --format='{{.State.Status}}' llama-server 2>/dev/null || true)
    if [[ "$state" == "running" ]]; then
        die "Container 'llama-server' is already running. Stop it first:
  docker compose -f $COMPOSE_FILE down"
    fi
}

check_model_file() {
    local id="$1"
    local model; model=$(get "$id" MODEL)
    local full_path="$MODELS_DIR/$model"
    if [[ ! -f "$full_path" ]]; then
        log "WARNING: Model file not found: $full_path — skipping $id"
        return 1
    fi
    return 0
}

# --- .env generation (from start.sh) ----------------------------------------

generate_env() {
    local id="$1"
    local model; model=$(get "$id" MODEL)
    local ctx; ctx=$(get "$id" CTX_SIZE)
    local ngl; ngl=$(get "$id" N_GPU_LAYERS)
    local fit; fit=$(get "$id" FIT)
    local extra; extra=$(get "$id" EXTRA_ARGS)

    {
        echo "# Generated by run-benchmark.sh — $(get "$id" NAME)"
        echo "# Section: [$id] from models.conf"
        echo ""
        [[ -n "$model" ]] && echo "MODEL=$model"
        [[ -n "$ctx" ]]   && echo "CTX_SIZE=$ctx"
        [[ -n "$ngl" ]]   && echo "N_GPU_LAYERS=$ngl"
        [[ -n "$fit" ]]   && echo "FIT=$fit"
        [[ -n "$extra" ]] && echo "EXTRA_ARGS=$extra"
    } > "$ENV_FILE"
}

# --- Health check (adapted from start.sh) -----------------------------------

wait_for_health() {
    local elapsed=0

    log "Waiting for server health (timeout: ${HEALTH_TIMEOUT}s)..."

    while (( elapsed < HEALTH_TIMEOUT )); do
        local state
        state=$(docker inspect --format='{{.State.Status}}' llama-server 2>/dev/null || echo "missing")
        if [[ "$state" != "running" ]]; then
            log "Container stopped unexpectedly. Recent logs:"
            docker compose -f "$COMPOSE_FILE" logs --tail=30 2>/dev/null || true
            return 1
        fi

        if curl -sf "$HEALTH_URL" &>/dev/null; then
            log "Server is ready."
            return 0
        fi

        sleep 5
        elapsed=$((elapsed + 5))
    done

    log "Timeout after ${HEALTH_TIMEOUT}s. Recent logs:"
    docker compose -f "$COMPOSE_FILE" logs --tail=20 2>/dev/null || true
    return 1
}

# --- Benchmark execution ----------------------------------------------------

run_codegen() {
    local model_id="$1"
    local model_name; model_name=$(get "$model_id" NAME)
    local output_dir="$RESULTS_DIR/$model_id"

    mkdir -p "$output_dir"

    log "Phase 1: Code generation for $model_name"

    # Run evalplus codegen from the venv
    "$VENV_DIR/bin/evalplus.codegen" \
        --model "$model_name" \
        --dataset humaneval \
        --base-url http://localhost:8080/v1 \
        --backend openai \
        --greedy \
        --root "$output_dir" \
        2>&1 | tee "$output_dir/codegen.log"

    local rc=${PIPESTATUS[0]}
    if [[ $rc -ne 0 ]]; then
        log "Code generation failed for $model_id (exit code $rc)"
        return 1
    fi

    log "Code generation complete. Output in $output_dir"
    return 0
}

run_evaluation() {
    local model_id="$1"
    local output_dir="$RESULTS_DIR/$model_id"

    log "Phase 2: Evaluation in Docker sandbox"

    # Find the generated samples file
    local samples_file
    samples_file=$(find "$output_dir/humaneval" -name "*.jsonl" ! -name "*.raw.jsonl" 2>/dev/null | head -1)

    if [[ -z "$samples_file" ]]; then
        log "No samples file found in $output_dir/humaneval/ — skipping evaluation"
        return 1
    fi

    log "Evaluating: $samples_file"

    # Run evaluation in Docker sandbox for safe code execution
    docker run --rm \
        -v "$output_dir:/app/results" \
        "$EVALPLUS_DOCKER" \
        evalplus.evaluate \
            --dataset humaneval \
            --samples "/app/results/humaneval/$(basename "$samples_file")" \
        2>&1 | tee "$output_dir/evaluation.log"

    local rc=${PIPESTATUS[0]}
    if [[ $rc -ne 0 ]]; then
        log "Evaluation failed for $model_id (exit code $rc)"
        return 1
    fi

    log "Evaluation complete for $model_id"
    return 0
}

# --- Report generation -------------------------------------------------------

generate_report() {
    if [[ ! -f "$REPORT_SCRIPT" ]]; then
        die "Report script not found: $REPORT_SCRIPT"
    fi
    "$VENV_DIR/bin/python" "$REPORT_SCRIPT" \
        --results-dir "$RESULTS_DIR" \
        --reference "$REFERENCE_FILE"
}

# =============================================================================
# Main
# =============================================================================

[[ ! -f "$CONF" ]] && die "Config file not found: $CONF"

parse_conf

# Collect bench-* model IDs
bench_ids=()
for id in "${SECTION_IDS[@]}"; do
    [[ "$id" == bench-* ]] && bench_ids+=("$id")
done

if [[ ${#bench_ids[@]} -eq 0 ]]; then
    die "No bench-* profiles found in $CONF"
fi

# Parse arguments
selected_ids=()
do_report=false

while [[ $# -gt 0 ]]; do
    case "$1" in
        --list|-l)
            list_bench_models
            exit 0
            ;;
        --report|-r)
            do_report=true
            shift
            ;;
        --help|-h)
            echo "Usage: $0 [bench-model-id ...] [--list] [--report] [--help]"
            echo ""
            echo "Options:"
            echo "  --list, -l     List available benchmark profiles"
            echo "  --report, -r   Generate comparison report from existing results"
            echo "  --help, -h     Show this help"
            echo ""
            echo "Examples:"
            echo "  $0                            # Run all bench-* models"
            echo "  $0 bench-glm-flash-q4         # Run one model"
            echo "  $0 bench-qwen3-coder bench-glm-flash  # Run specific models"
            echo "  $0 --report                   # Report only (no benchmarking)"
            exit 0
            ;;
        -*)
            die "Unknown option: $1"
            ;;
        *)
            # Validate model ID
            found=false
            for id in "${bench_ids[@]}"; do
                if [[ "$id" == "$1" ]]; then
                    selected_ids+=("$1")
                    found=true
                    break
                fi
            done
            if [[ "$found" == false ]]; then
                echo "Error: Unknown benchmark profile '$1'" >&2
                echo ""
                list_bench_models >&2
                exit 1
            fi
            shift
            ;;
    esac
done

# Report-only mode
if [[ "$do_report" == true && ${#selected_ids[@]} -eq 0 ]]; then
    check_prerequisites
    generate_report
    exit 0
fi

# Default: run all bench-* models
if [[ ${#selected_ids[@]} -eq 0 ]]; then
    selected_ids=("${bench_ids[@]}")
fi

# Full run: prerequisites check
check_prerequisites
check_no_running_container

# Pull evalplus Docker image for sandbox evaluation
log "Ensuring evalplus Docker sandbox image is available..."
docker pull "$EVALPLUS_DOCKER"

mkdir -p "$RESULTS_DIR"

# Summary counters
total=0
passed=0
failed=0
skipped=0

log "Starting benchmark run: ${#selected_ids[@]} model(s)"
log "Models: ${selected_ids[*]}"
echo ""

for model_id in "${selected_ids[@]}"; do
    total=$((total + 1))
    echo "================================================================"
    log "[$total/${#selected_ids[@]}] $(get "$model_id" NAME)"
    echo "================================================================"

    # Check model file
    if ! check_model_file "$model_id"; then
        skipped=$((skipped + 1))
        continue
    fi

    # Generate .env and start container
    generate_env "$model_id"
    log "Starting container..."
    docker compose -f "$COMPOSE_FILE" up -d

    # Wait for health
    if ! wait_for_health; then
        log "Server failed to start for $model_id — skipping"
        docker compose -f "$COMPOSE_FILE" down 2>/dev/null || true
        failed=$((failed + 1))
        echo ""
        continue
    fi

    # Phase 1: Generate code
    codegen_ok=true
    if ! run_codegen "$model_id"; then
        codegen_ok=false
        failed=$((failed + 1))
    fi

    # Stop container (no longer needed after generation)
    log "Stopping container..."
    docker compose -f "$COMPOSE_FILE" down 2>/dev/null || true

    # Phase 2: Evaluate in Docker sandbox (only if codegen succeeded)
    if [[ "$codegen_ok" == true ]]; then
        if run_evaluation "$model_id"; then
            passed=$((passed + 1))
        else
            failed=$((failed + 1))
        fi
    fi

    echo ""
done

# Summary
echo "================================================================"
log "Benchmark run complete"
log "  Total: $total | Passed: $passed | Failed: $failed | Skipped: $skipped"
echo "================================================================"

# Generate report if any results exist
if [[ $passed -gt 0 ]]; then
    echo ""
    log "Generating comparison report..."
    generate_report || log "Report generation failed (non-fatal)"
fi

# =============================================================================
# bench-client.conf — Client-side config for benchmark code generation
#
# These settings are sent BY the client (codegen) TO the model API.
# They do NOT affect server startup — that's models.conf.
#
# [defaults] applies to ALL models. Per-model sections override defaults.
#
# When SYSTEM_PROMPT is set for a model, codegen-custom.py is used instead
# of evalplus.codegen, because evalplus doesn't support system prompts.
#
# MAX_TOKENS is patched into evalplus automatically at benchmark start
# (evalplus has no CLI flag for this). Also passed to codegen-custom.py.
# =============================================================================


# Global defaults for all benchmark models
[defaults]
MAX_TOKENS=4096


# GPT-OSS 120B — reasoning level via system prompt trigger.
# Options: "Reasoning: low", "Reasoning: medium", "Reasoning: high"
# See models/documentation/README_modelcard_gpt-oss-120b-GGUF.md
[bench-gpt-oss-120b]
SYSTEM_PROMPT=Reasoning: high

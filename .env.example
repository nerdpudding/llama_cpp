# =============================================================================
# .env template â€” llama.cpp Docker wrapper
#
# Copy a model-specific config instead of editing this file:
#   cp .env.qwen3-coder .env     # Qwen3-Coder-Next UD-Q6_K_XL (recommended)
#   cp .env.glm-flash .env       # GLM-4.7 Flash Q8_0
#   cp .env.gpt-oss-120b .env    # GPT-OSS 120B
#   cp .env.qwen3-coder-q6k .env # Qwen3-Coder-Next Q6_K
#
# Or customize below for a new model.
# =============================================================================

# Path to the GGUF model file, relative to the models/ directory.
# For multi-part models, point to the first file (-00001-of-0000X.gguf).
MODEL=model.gguf

# Context window size in tokens.
CTX_SIZE=131072

# Number of model layers to offload to GPU. 99 = all layers.
N_GPU_LAYERS=99

# Auto-fit: automatically adjusts layers/context to fit available VRAM.
# Set to "off" when using -ot tensor overrides (auto-fit doesn't account for
# the expert/attention split correctly).
FIT=on

# Extra flags passed directly to llama-server.
# Common flags:
#   --jinja              Enable Jinja chat templates (required for most models)
#   -np 1                Single parallel slot (1 KV cache, saves memory)
#   -b 2048 -ub 2048     Batch/micro-batch size for prompt processing
#   --no-context-shift   Stop at context limit instead of shifting out old tokens
#   --temp 1.0           Sampling temperature
#   --top-p 0.95         Top-p sampling
#   --top-k 40           Top-k sampling
#   --min-p 0            Min-p sampling
#   -ot <regex>=<device> Tensor override for per-layer GPU/CPU placement
EXTRA_ARGS=--jinja -np 1

# =============================================================================
# Advanced settings (usually left at defaults in docker-compose.yml):
#
# SPLIT_MODE=layer          Multi-GPU split mode: layer, row, none
# TENSOR_SPLIT=             Manual split ratio (blank = auto)
# MAIN_GPU=0                Primary GPU index (0 = RTX 4090)
# FLASH_ATTN=1              Flash attention (1 = on)
# KV_CACHE_TYPE_K=q8_0      KV cache key quantization
# KV_CACHE_TYPE_V=q8_0      KV cache value quantization
# FIT_TARGET=               VRAM headroom per device in MiB
# FIT_CTX=                  Minimum context size for auto-fit
# =============================================================================

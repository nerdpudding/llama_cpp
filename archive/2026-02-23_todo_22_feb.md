# TODO 22-23 February

## Completed (22 Feb)

### Check llama.cpp for upstream updates
- [x] Check current local llama.cpp version/commit — `b48e80f67` (b8022, 13 Feb)
- [x] Check latest upstream commits — 93 new commits to `ed4837891`
- [x] Review what changed — relevant: qwen3next graph opt, CUDA graph improvements, GPT-OSS Jinja fix
- [x] Updated, rebuilt, tested — CUDA illegal memory access regression on qwen3next models with `-ot` multi-GPU splits
- [x] Reverted to `b48e80f67`, rebuilt, tested — works correctly
- [x] Filed upstream issue: https://github.com/ggml-org/llama.cpp/issues/19816
- [x] Bench profile `bench-qwen3-next-ud-q5` verified working on reverted build (~29.5 t/s)

### Run Qwen3-Next benchmark
- [x] Run EvalPlus HumanEval+ benchmark for `bench-qwen3-next-ud-q5`
- [x] Review results — 98.2% HumanEval, 93.9% HumanEval+ (surprisingly high for a general model)
- [x] Merge into `benchmarks/evalplus/results/REPORT.md` (automatic via generate-report.py)
- [x] Update README.md benchmark table with Qwen3-Next scores

## Completed (23 Feb)

### Bisect and fix CUDA regression
- [x] Bisected 93 commits using `git bisect` — found `1725e316c` (PR #19375, qwen3next graph rewrite)
- [x] Tested `GGML_CUDA_DISABLE_GRAPHS=1` on latest — same crash, ruled out CUDA graphs
- [x] ggerganov responded with one-line fix: `ggml_set_inplace` → `ggml_set`
- [x] Applied patch, tested on `ed4837891` — fix confirmed, long prompts work
- [x] Posted bisect result and fix confirmation on issue #19816
- [x] Now running `ed4837891` with local patch, waiting for upstream merge
- [x] Speed check: Qwen3-Coder-Next ~29.5 t/s on patched build (same as before, no regression)
- [x] Picked roadmap item: model switching + dashboard decoupling

## Ongoing

### llama.cpp upstream merge
- Waiting for `ggml_set_inplace` → `ggml_set` fix to merge into master
- Running patched `ed4837891` locally in the meantime
- Full details: `docs/known_issue_llama_cpp_cuda_graphs_2026-02-22.md`

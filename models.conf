# =============================================================================
# models.conf — Central model configuration for llama.cpp Docker wrapper
#
# Each [section] defines a model. start.sh reads this file and generates .env.
#
# Required fields: MODEL
# Optional fields: CTX_SIZE, N_GPU_LAYERS, FIT, EXTRA_ARGS
# (docker-compose.yml defaults apply when omitted)
#
# Section IDs are used as CLI shortcuts: ./start.sh qwen3-coder
# =============================================================================


# -----------------------------------------------------------------------------
# GLM-4.7 Flash — fast, compact model that fits entirely in VRAM
# 128K native context window. No CPU offloading needed.
# -----------------------------------------------------------------------------

[glm-flash-q4]
NAME=GLM-4.7 Flash Q4_K_M
MODEL=GLM-4.7-Flash/GLM-4.7-Flash-Q4_K_M.gguf
CTX_SIZE=131072
N_GPU_LAYERS=99
FIT=on
EXTRA_ARGS=--jinja -np 1 --temp 1.0 --top-p 0.95 --min-p 0.01

[glm-flash]
NAME=GLM-4.7 Flash Q8_0
MODEL=GLM-4.7-Flash/GLM-4.7-Flash-Q8_0.gguf
CTX_SIZE=131072
N_GPU_LAYERS=99
FIT=on
EXTRA_ARGS=--jinja -np 1 --temp 1.0 --top-p 0.95 --min-p 0.01

[glm-flash-exp]
NAME=GLM-4.7 Flash Q8_0 (experimental)
MODEL=GLM-4.7-Flash/other/GLM-4.7-Flash-experimental.Q8_0.gguf
CTX_SIZE=131072
N_GPU_LAYERS=99
FIT=on
EXTRA_ARGS=--jinja -np 1


# -----------------------------------------------------------------------------
# GPT-OSS 120B F16 (MXFP4 native) — MoE with CPU expert offloading
#
# ~20 t/s at 64K context. 116.8B total params, ~5.1B active per token.
# Layers 0-11 on RTX 4090, layers 12-15 on RTX 5070 Ti, remaining on CPU.
# VRAM: 4090 at 96%, 5070 Ti at 89%.
#
# Memory breakdown:
#   Model weights on GPU: ~29 GB (across both GPUs)
#   Model weights on CPU: ~61 GB (mmap)
#   KV cache (64K, q8_0): ~1.3 GB
#   Compute buffers: ~6.2 GB
#
# See docs/gpt-oss-120b-configuration-guide.md for full details.
# -----------------------------------------------------------------------------

[gpt-oss-120b]
NAME=GPT-OSS 120B F16
MODEL=GPT-OSS-120b/gpt-oss-120b-F16.gguf
CTX_SIZE=65536
N_GPU_LAYERS=99
FIT=off
EXTRA_ARGS=--jinja -np 1 --temp 1.0 --top-p 1.0 -b 4096 -ub 4096 -ot blk\.([0-9]|1[01])\.=CUDA0,blk\.(1[2-5])\.=CUDA1,exps=CPU


# -----------------------------------------------------------------------------
# Qwen3-Coder-Next — 80B MoE (512 experts, 10 active, ~3B active/token)
#
# 75% DeltaNet (linear attention, no KV cache) + 25% standard attention.
# Only 12/48 layers have KV cache, making 256K context feasible.
#
# Unsloth Dynamic (UD) quants give higher precision to expert router tensors,
# resulting in better expert selection and no self-correction behavior.
#
# See docs/llama-cpp-flags-and-qwen3-strategy.md for full details.
# -----------------------------------------------------------------------------

# Speed option: 25.8 t/s, 15+7 layers on GPU (22/48)
# Strategy 3 from docs — same as UD-Q6_K_XL but +21% faster
[qwen3-coder-q5]
NAME=Qwen3-Coder-Next UD-Q5_K_XL (speed)
MODEL=Qwen3-Coder-Next/UD-Q5_K_XL/Qwen3-Coder-Next-UD-Q5_K_XL-00001-of-00003.gguf
CTX_SIZE=262144
N_GPU_LAYERS=99
FIT=off
EXTRA_ARGS=--jinja -np 1 -b 2048 -ub 2048 --no-context-shift --temp 1.0 --top-p 0.95 --top-k 40 --min-p 0.01 -ot blk\.([0-9]|1[0-4])\.=CUDA0,blk\.(1[5-9]|2[0-1])\.=CUDA1,exps=CPU

# Baseline: 21.4 t/s, 13+6 layers on GPU (19/48), best quality
# Strategy 5 from docs — recommended default
[qwen3-coder]
NAME=Qwen3-Coder-Next UD-Q6_K_XL (baseline)
MODEL=Qwen3-Coder-Next/UD-Q6_K_XL/Qwen3-Coder-Next-UD-Q6_K_XL-00001-of-00003.gguf
CTX_SIZE=262144
N_GPU_LAYERS=99
FIT=off
EXTRA_ARGS=--jinja -np 1 -b 2048 -ub 2048 --no-context-shift --temp 1.0 --top-p 0.95 --top-k 40 --min-p 0.01 -ot blk\.([0-9]|1[0-2])\.=CUDA0,blk\.(1[3-8])\.=CUDA1,exps=CPU

# Standard Q6_K (non-UD): ~21 t/s, 13+6 layers on GPU (19/48)
# Same layer split as UD-Q6_K_XL since model sizes are similar.
# For most use cases, prefer qwen3-coder (UD-Q6_K_XL) for better quality.
[qwen3-coder-q6k]
NAME=Qwen3-Coder-Next Q6_K
MODEL=Qwen3-Coder-Next/Q6_K/Qwen3-Coder-Next-Q6_K-00001-of-00003.gguf
CTX_SIZE=262144
N_GPU_LAYERS=99
FIT=off
EXTRA_ARGS=--jinja -np 1 -b 2048 -ub 2048 --no-context-shift --temp 1.0 --top-p 0.95 --top-k 40 --min-p 0.01 -ot blk\.([0-9]|1[0-2])\.=CUDA0,blk\.(1[3-8])\.=CUDA1,exps=CPU


# =============================================================================
# Benchmark profiles (EvalPlus HumanEval+)
#
# Reduced context (16K) for faster loading and less VRAM waste.
# Same model files and -ot layer splits as production profiles.
# EvalPlus sends temperature=0 via API (--greedy), overriding server defaults.
#
# Used by: benchmarks/evalplus/run-benchmark.sh
# =============================================================================


# --- GLM-4.7 Flash benchmarks ---

[bench-glm-flash-q4]
NAME=GLM-4.7 Flash Q4_K_M (benchmark)
MODEL=GLM-4.7-Flash/GLM-4.7-Flash-Q4_K_M.gguf
CTX_SIZE=16384
N_GPU_LAYERS=99
FIT=on
EXTRA_ARGS=--jinja -np 1

[bench-glm-flash]
NAME=GLM-4.7 Flash Q8_0 (benchmark)
MODEL=GLM-4.7-Flash/GLM-4.7-Flash-Q8_0.gguf
CTX_SIZE=16384
N_GPU_LAYERS=99
FIT=on
EXTRA_ARGS=--jinja -np 1


# --- GPT-OSS 120B benchmark ---

[bench-gpt-oss-120b]
NAME=GPT-OSS 120B F16 (benchmark)
MODEL=GPT-OSS-120b/gpt-oss-120b-F16.gguf
CTX_SIZE=16384
N_GPU_LAYERS=99
FIT=off
EXTRA_ARGS=--jinja -np 1 -b 4096 -ub 4096 -ot blk\.([0-9]|1[01])\.=CUDA0,blk\.(1[2-5])\.=CUDA1,exps=CPU


# --- Qwen3-Coder-Next benchmarks ---

[bench-qwen3-coder-q5]
NAME=Qwen3-Coder-Next UD-Q5_K_XL (benchmark)
MODEL=Qwen3-Coder-Next/UD-Q5_K_XL/Qwen3-Coder-Next-UD-Q5_K_XL-00001-of-00003.gguf
CTX_SIZE=16384
N_GPU_LAYERS=99
FIT=off
EXTRA_ARGS=--jinja -np 1 -b 2048 -ub 2048 --no-context-shift -ot blk\.([0-9]|1[0-4])\.=CUDA0,blk\.(1[5-9]|2[0-1])\.=CUDA1,exps=CPU

[bench-qwen3-coder]
NAME=Qwen3-Coder-Next UD-Q6_K_XL (benchmark)
MODEL=Qwen3-Coder-Next/UD-Q6_K_XL/Qwen3-Coder-Next-UD-Q6_K_XL-00001-of-00003.gguf
CTX_SIZE=16384
N_GPU_LAYERS=99
FIT=off
EXTRA_ARGS=--jinja -np 1 -b 2048 -ub 2048 --no-context-shift -ot blk\.([0-9]|1[0-2])\.=CUDA0,blk\.(1[3-8])\.=CUDA1,exps=CPU

[bench-qwen3-coder-q6k]
NAME=Qwen3-Coder-Next Q6_K (benchmark)
MODEL=Qwen3-Coder-Next/Q6_K/Qwen3-Coder-Next-Q6_K-00001-of-00003.gguf
CTX_SIZE=16384
N_GPU_LAYERS=99
FIT=off
EXTRA_ARGS=--jinja -np 1 -b 2048 -ub 2048 --no-context-shift -ot blk\.([0-9]|1[0-2])\.=CUDA0,blk\.(1[3-8])\.=CUDA1,exps=CPU

# =============================================================================
# models.conf — Server configuration for llama.cpp Docker wrapper
#
# This file controls HOW THE SERVER STARTS: model file, context size, GPU
# layers, and llama-server flags. start.sh and benchmark.sh read this file
# and generate .env → docker-compose.yml → llama-server.
#
# Client-side settings (system prompts, reasoning levels, etc.) are NOT here.
# For benchmarks, see benchmarks/evalplus/bench-client.conf.
# For interactive use, set system prompts in your client (web UI, API calls).
#
# Required fields: MODEL
# Optional fields: CTX_SIZE, N_GPU_LAYERS, FIT, EXTRA_ARGS
# (docker-compose.yml defaults apply when omitted)
#
# Section IDs are used as CLI shortcuts: ./start.sh qwen3-coder
# =============================================================================


# -----------------------------------------------------------------------------
# GLM-4.7 Flash — 30B-A3B MoE (64 experts, 4 active + 1 shared per layer)
#
# 47 layers (1 dense lead + 46 MoE). 128K native context window.
# MLA (Multi-head Latent Attention): compact KV cache (~3.5 GB at 128K).
# Q4 (18 GB) fits entirely on RTX 4090 even at 128K (Strategy A).
# Q8 (30 GB) needs both GPUs with explicit -ot placement (Strategy C).
#
# Architecture source: model card (models/documentation/README_modelcard_GLM-4.7-Flash.md)
# See docs/gpu-strategy-guide.md for GPU placement decision tree.
#
# All production profiles optimized 2026-02-16. Key insight: -ub 512
# (llama.cpp default) saves significant compute buffer VRAM vs -ub 2048.
# See docs/gpu-strategy-guide.md "Batch size and VRAM" and
# docs/lessons_learned.md #5 for details.
# -----------------------------------------------------------------------------

# Q4: Strategy A — all on RTX 4090. Optimized 2026-02-16.
# Model (18 GB) + KV (3.5 GB) + compute buffer fits within 24 GB.
# --split-mode none prevents unnecessary distribution to CUDA1.
# CUDA0 88%, 2 graph splits, 142.7 t/s. Previous: FIT=on.
[glm-flash-q4]
NAME=GLM-4.7 Flash Q4_K_M
MODEL=GLM-4.7-Flash/GLM-4.7-Flash-Q4_K_M.gguf
CTX_SIZE=131072
N_GPU_LAYERS=99
FIT=off
EXTRA_ARGS=--jinja -np 1 --temp 1.0 --top-p 0.95 --min-p 0.01 --split-mode none --main-gpu 0

# Q8: Strategy C — explicit 33+14=47/47 split, all layers on GPU.
# At 128K, KV + compute buffers grow ~5 GB vs bench. Shifted 2 layers from
# CUDA0 to CUDA1 vs bench split (35+12) to make room. -ub 512 keeps compute
# buffer small (~448 MiB vs 897 MiB at -ub 1024, which OOMed by 372 MiB).
# Expected: CUDA0 ~95%, CUDA1 ~80%. ~105 t/s if graph splits are low.
# Previous: FIT=on (auto-distributed without MoE-aware priorities).
[glm-flash-q8]
NAME=GLM-4.7 Flash Q8_0
MODEL=GLM-4.7-Flash/GLM-4.7-Flash-Q8_0.gguf
CTX_SIZE=131072
N_GPU_LAYERS=99
FIT=off
EXTRA_ARGS=--jinja -np 1 --temp 1.0 --top-p 0.95 --min-p 0.01 -b 2048 -ub 512 -ot blk\.([0-9]|[12][0-9]|3[0-2])\.=CUDA0,blk\.(3[3-9]|4[0-6])\.=CUDA1

# Q8 experimental: same optimization as Q8, different model file.
# Previous: FIT=on.
[glm-flash-exp]
NAME=GLM-4.7 Flash Q8_0 (experimental)
MODEL=GLM-4.7-Flash/other/GLM-4.7-Flash-experimental.Q8_0.gguf
CTX_SIZE=131072
N_GPU_LAYERS=99
FIT=off
EXTRA_ARGS=--jinja -np 1 -b 2048 -ub 512 -ot blk\.([0-9]|[12][0-9]|3[0-2])\.=CUDA0,blk\.(3[3-9]|4[0-6])\.=CUDA1


# -----------------------------------------------------------------------------
# GPT-OSS 120B F16 (MXFP4 native) — MoE with CPU expert offloading
#
# 116.8B total params, ~5.1B active per token. 36 layers (~1.7 GB/layer).
# 128 experts per layer, 4 active. Full KV cache on all 36 layers.
# Strategy D: GPU + CPU offload, experts on CPU for overflow layers.
# Split: 11 CUDA0 + 4 CUDA1 = 15/36. Optimized 2026-02-16.
# Previous: 11+3=14/36, -b 2048 -ub 2048 (wasted ~1.2 GB compute buffer/GPU).
# Switching to -ub 512 freed room for +1 layer on CUDA1. Speed: ~20 t/s.
# CUDA0 90%, CUDA1 75%. 68 graph splits (bs=1).
#
# Reasoning levels: GPT-OSS uses a system prompt trigger, not a server flag.
# Set "Reasoning: low/medium/high" in the system prompt of your client (web UI,
# API call, etc.). llama-server's --system-prompt flag is excluded from the
# server binary, so this CANNOT be set via EXTRA_ARGS or server config.
# See model card (models/documentation/README_modelcard_gpt-oss-120b-GGUF.md).
# -----------------------------------------------------------------------------

[gpt-oss-120b]
NAME=GPT-OSS 120B F16
MODEL=GPT-OSS-120b/gpt-oss-120b-F16.gguf
CTX_SIZE=131072
N_GPU_LAYERS=99
FIT=off
EXTRA_ARGS=--jinja -np 1 --temp 1.0 --top-p 1.0 -b 2048 -ub 512 -ot blk\.([0-9]|10)\.=CUDA0,blk\.(1[1-4])\.=CUDA1,exps=CPU


# -----------------------------------------------------------------------------
# Qwen3-Coder-Next — 80B MoE (512 experts, 10 active, ~3B active/token)
#
# 48 layers. 75% DeltaNet (linear attention, no KV cache) + 25% standard
# attention. Only 12/48 layers have KV cache, making 256K context feasible.
# KV cache at 256K: ~3.1 GB (vs ~128 MB at 10K bench context).
#
# Unsloth Dynamic (UD) quants give higher precision to expert router tensors,
# resulting in better expert selection and no self-correction behavior.
# Benchmark results: UD-Q5 scores higher than UD-Q6 (93.9% vs 92.1%
# HumanEval) while being 21% faster (25.8 vs 21.4 t/s). UD-Q5 is the
# recommended production quant.
#
# See models/documentation/README_modelcard_qwen3_coder_next.md and
# docs/gpu-strategy-guide.md.
#
# Split: 17 CUDA0 + 8 CUDA1 = 25/48. Optimized 2026-02-16.
# Previous: 15+7=22/48, -b 2048 -ub 2048 (wasted compute buffer VRAM).
# Switching to -ub 512 + more layers: +8% speed (27.9 t/s vs 25.8 t/s).
# CUDA0 ~95%, CUDA1 ~86%. 130 graph splits (bs=1).
# -----------------------------------------------------------------------------

# Primary coding model: UD-Q5 is both faster and higher scoring than UD-Q6.
# Strategy D: GPU + CPU offload, experts on CPU for overflow layers.
[qwen3-coder-q5]
NAME=Qwen3-Coder-Next UD-Q5_K_XL
MODEL=Qwen3-Coder-Next/UD-Q5_K_XL/Qwen3-Coder-Next-UD-Q5_K_XL-00001-of-00003.gguf
CTX_SIZE=262144
N_GPU_LAYERS=99
FIT=off
EXTRA_ARGS=--jinja -np 1 -b 2048 -ub 512 --no-context-shift --temp 1.0 --top-p 0.95 --top-k 40 --min-p 0.01 -ot blk\.([0-9]|1[0-6])\.=CUDA0,blk\.(1[7-9]|2[0-4])\.=CUDA1,exps=CPU

# UD-Q6: kept for reference but NOT optimized. UD-Q5 is strictly better on
# all metrics (speed, HumanEval score, HumanEval+ score).
[qwen3-coder]
NAME=Qwen3-Coder-Next UD-Q6_K_XL (baseline)
MODEL=Qwen3-Coder-Next/UD-Q6_K_XL/Qwen3-Coder-Next-UD-Q6_K_XL-00001-of-00003.gguf
CTX_SIZE=262144
N_GPU_LAYERS=99
FIT=off
EXTRA_ARGS=--jinja -np 1 -b 2048 -ub 512 --no-context-shift --temp 1.0 --top-p 0.95 --top-k 40 --min-p 0.01 -ot blk\.([0-9]|1[0-2])\.=CUDA0,blk\.(1[3-8])\.=CUDA1,exps=CPU


# =============================================================================
# Benchmark profiles (EvalPlus HumanEval+) — FINALIZED, DO NOT MODIFY
#
# These profiles are fully optimized and tested (OOM-verified 2026-02-15).
# Full benchmark run completed with these exact settings — results in
# benchmarks/evalplus/results/REPORT.md. Changing these would invalidate
# the benchmark data.
#
# Optimized for benchmarks: 10K context (HumanEval worst case is ~8.4K tokens),
# small batch sizes (prompts are ~400 tokens max), and more GPU layers than
# production (less KV cache + smaller compute buffers = more room for weights).
#
# Key differences from production profiles:
#   - CTX_SIZE=10240 (vs 128K/256K) — saves GB of KV cache
#   - -b 512 -ub 512 (vs 1024-2048) — HumanEval prompts are tiny, smaller
#     micro-batch reduces compute buffer VRAM by several GB
#   - More layers on GPU — freed VRAM used for model weights = faster inference
#   - No sampler args — evalplus sends temperature=0 via API (greedy decoding)
#
# Client-side config (system prompts) is in benchmarks/evalplus/bench-client.conf.
# Used by: benchmarks/evalplus/benchmark.sh
# =============================================================================


# --- GLM-4.7 Flash benchmarks ---
# MoE model: 30B-A3B (64 experts, 4 active + 1 shared, 47 layers).
# Architecture source: model card (models/documentation/README_modelcard_GLM-4.7-Flash.md)
#
# GPU strategy (per docs/gpu-strategy-guide.md):
#   Q4 (18 GB file, ~17.5 GB on GPU): fits entirely on RTX 4090 at 10K context.
#     → Strategy A: single GPU, --split-mode none --main-gpu 0
#   Q8 (30 GB file, ~30 GB on GPU): exceeds 4090, needs both GPUs.
#     → Strategy C: selective GPU split with -ot, no exps=CPU needed because
#       30 GB fits within combined GPU VRAM (4090 24 GB + 5070 Ti 12.5 GB usable).
#       Maximize layers on CUDA0, overflow to CUDA1.
#
# --reasoning-format none: GLM is a thinking model. Without this flag, the
# chain-of-thought goes into a separate reasoning_content field that evalplus
# ignores, resulting in empty solutions. With this flag, everything (thinking +
# answer) goes into the content field. The postprocessor then strips the
# <think> tags, leaving only the code. Production profiles don't need this
# because the web UI renders reasoning_content natively.

# Q4: all on RTX 4090 (Strategy A). Verified: 17,285 MiB model buffer,
# 18,361 MiB total VRAM = 74% of 4090. No inter-GPU transfers, graph splits = 2
# (CPU↔GPU0 only for tok_embd, which llama.cpp always keeps on CPU).
# Measured: ~140 t/s.
#
# Old broken config (before 2026-02-15): used FIT=on without --split-mode none,
# which caused llama.cpp to unnecessarily split the 18 GB model across both GPUs.
# This added inter-GPU transfers for no benefit — Q4 fits entirely on the 4090.
#   FIT=on
#   EXTRA_ARGS=--jinja -np 1 --reasoning-format none
[bench-glm-flash-q4]
NAME=GLM-4.7 Flash Q4_K_M (benchmark)
MODEL=GLM-4.7-Flash/GLM-4.7-Flash-Q4_K_M.gguf
CTX_SIZE=10240
N_GPU_LAYERS=99
FIT=off
EXTRA_ARGS=--jinja -np 1 --reasoning-format none --split-mode none --main-gpu 0

# Q8: split across both GPUs (Strategy C). 30 GB model needs both GPUs but
# fits within combined VRAM, so no exps=CPU needed — all experts stay on GPU.
# 47 layers at ~628 MiB/layer (measured).
#
# Tested 35+12: 33 graph splits, ~105 t/s. CUDA0: 22.0/24.6 GB (headroom ~1.8 GB).
# Tested 37+10: 53 graph splits, ~102 t/s. More splits = slower despite more on 4090.
# The split boundary affects MoE graph scheduling — fewer splits wins over more
# layers on the faster GPU. See docs/gpu-strategy-guide.md "Graph splits" section.
#
# 35+12 is the sweet spot: fewest splits + most on fastest GPU.
# Tested 37+10 (slower, more splits):
#   EXTRA_ARGS=... -ot blk\.([0-9]|[12][0-9]|3[0-6])\.=CUDA0,blk\.(3[7-9]|4[0-6])\.=CUDA1
[bench-glm-flash-q8]
NAME=GLM-4.7 Flash Q8_0 (benchmark)
MODEL=GLM-4.7-Flash/GLM-4.7-Flash-Q8_0.gguf
CTX_SIZE=10240
N_GPU_LAYERS=99
FIT=off
EXTRA_ARGS=--jinja -np 1 --reasoning-format none -b 512 -ub 512 -ot blk\.([0-9]|[12][0-9]|3[0-4])\.=CUDA0,blk\.(3[5-9]|4[0-6])\.=CUDA1


# --- GPT-OSS 120B benchmark ---
# MoE, 36 layers (~1.7 GB/layer). Production: 12 CUDA0 + 4 CUDA1 at 64K/-ub 4096.
# At 10K/-ub 512: KV + compute buffers shrink, freeing VRAM for more GPU layers.
#
# Tested 13+5=18/36: OK, ~22 t/s. CUDA0 96%, CUDA1 83%.
# Tested 13+6=19/36: OOM on CUDA1 at inference (15.4/16.3 GB after load, no room
# for runtime allocations). 13+5 is the max for this model.
#
# Previous splits:
#   13+6 (OOM CUDA1):    -ot blk\.([0-9]|1[0-2])\.=CUDA0,blk\.(1[3-8])\.=CUDA1,exps=CPU
#   12+5 (original):     -ot blk\.([0-9]|1[01])\.=CUDA0,blk\.(1[2-6])\.=CUDA1,exps=CPU

[bench-gpt-oss-120b]
NAME=GPT-OSS 120B F16 (benchmark)
MODEL=GPT-OSS-120b/gpt-oss-120b-F16.gguf
CTX_SIZE=10240
N_GPU_LAYERS=99
FIT=off
EXTRA_ARGS=--jinja -np 1 -b 512 -ub 512 -ot blk\.([0-9]|1[0-2])\.=CUDA0,blk\.(1[3-7])\.=CUDA1,exps=CPU


# --- Qwen3-Coder-Next benchmarks ---
# MoE with DeltaNet: only 12/48 layers have KV cache. Production at 256K:
# KV ~3.2 GB. At 10K: ~128 MB (saves ~3 GB!). Combined with -ub 512 (saves
# compute buffer), total ~4-5 GB freed per GPU. Each layer ~1.3 GB (Q6).
#
# Q5 production: 15 CUDA0 + 7 CUDA1 = 22/48.
# Tested 18+8=26/48: OK, ~29 t/s. CUDA0 89% (~2.7 GB over), CUDA1 81% (~3 GB over).
# Both GPUs have room for +1 layer (~1.1 GB/layer) → trying 19+9=28/48.
#
# Previous split (18+8, OK ~29 t/s):
#   -ot blk\.([0-9]|1[0-7])\.=CUDA0,blk\.(1[8-9]|2[0-5])\.=CUDA1,exps=CPU

[bench-qwen3-coder-ud-q5]
NAME=Qwen3-Coder-Next UD-Q5_K_XL (benchmark)
MODEL=Qwen3-Coder-Next/UD-Q5_K_XL/Qwen3-Coder-Next-UD-Q5_K_XL-00001-of-00003.gguf
CTX_SIZE=10240
N_GPU_LAYERS=99
FIT=off
EXTRA_ARGS=--jinja -np 1 -b 512 -ub 512 --no-context-shift -ot blk\.([0-9]|1[0-8])\.=CUDA0,blk\.(1[9]|2[0-7])\.=CUDA1,exps=CPU

# UD-Q6: layers ~1.33 GB each (bigger than Q5's ~1.1 GB).
# 17+8=25/48 likely OOM (load loop). Reduced CUDA0 by 1 → 16+8=24/48.
# Previous splits:
#   17+8 (OOM?):  -ot blk\.([0-9]|1[0-6])\.=CUDA0,blk\.(1[7-9]|2[0-4])\.=CUDA1,exps=CPU
#   16+7 (orig):  -ot blk\.([0-9]|1[0-5])\.=CUDA0,blk\.(1[6-9]|2[0-2])\.=CUDA1,exps=CPU
[bench-qwen3-coder-ud-q6]
NAME=Qwen3-Coder-Next UD-Q6_K_XL (benchmark)
MODEL=Qwen3-Coder-Next/UD-Q6_K_XL/Qwen3-Coder-Next-UD-Q6_K_XL-00001-of-00003.gguf
CTX_SIZE=10240
N_GPU_LAYERS=99
FIT=off
EXTRA_ARGS=--jinja -np 1 -b 512 -ub 512 --no-context-shift -ot blk\.([0-9]|1[0-5])\.=CUDA0,blk\.(1[6-9]|2[0-3])\.=CUDA1,exps=CPU


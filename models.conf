# =============================================================================
# models.conf — Server configuration for llama.cpp Docker wrapper
#
# This file controls HOW THE SERVER STARTS: model file, context size, GPU
# layers, and llama-server flags. start.sh and benchmark.sh read this file
# and generate .env → docker-compose.yml → llama-server.
#
# Client-side settings (system prompts, reasoning levels, etc.) are NOT here.
# For benchmarks, see benchmarks/evalplus/bench-client.conf.
# For interactive use, set system prompts in your client (web UI, API calls).
#
# Required fields: MODEL
# Optional fields: CTX_SIZE, N_GPU_LAYERS, FIT, EXTRA_ARGS
# (docker-compose.yml defaults apply when omitted)
#
# Section IDs are used as CLI shortcuts: ./start.sh qwen3-coder
# =============================================================================


# -----------------------------------------------------------------------------
# GLM-4.7 Flash — 30B-A3B MoE (64 experts, 4 active + 1 shared per layer)
#
# 47 layers (1 dense lead + 46 MoE). 128K native context window.
# Q4 (18 GB) fits entirely on RTX 4090. Q8 (30 GB) needs both GPUs or CPU.
# FIT=on works for production because the model is small enough that
# auto-distribution across GPUs is acceptable for interactive use.
#
# Architecture source: model card (models/documentation/README_modelcard_GLM-4.7-Flash.md)
# See docs/gpu-strategy-guide.md for GPU placement decision tree.
# -----------------------------------------------------------------------------

[glm-flash-q4]
NAME=GLM-4.7 Flash Q4_K_M
MODEL=GLM-4.7-Flash/GLM-4.7-Flash-Q4_K_M.gguf
CTX_SIZE=131072
N_GPU_LAYERS=99
FIT=on
EXTRA_ARGS=--jinja -np 1 --temp 1.0 --top-p 0.95 --min-p 0.01

[glm-flash]
NAME=GLM-4.7 Flash Q8_0
MODEL=GLM-4.7-Flash/GLM-4.7-Flash-Q8_0.gguf
CTX_SIZE=131072
N_GPU_LAYERS=99
FIT=on
EXTRA_ARGS=--jinja -np 1 --temp 1.0 --top-p 0.95 --min-p 0.01

[glm-flash-exp]
NAME=GLM-4.7 Flash Q8_0 (experimental)
MODEL=GLM-4.7-Flash/other/GLM-4.7-Flash-experimental.Q8_0.gguf
CTX_SIZE=131072
N_GPU_LAYERS=99
FIT=on
EXTRA_ARGS=--jinja -np 1


# -----------------------------------------------------------------------------
# GPT-OSS 120B F16 (MXFP4 native) — MoE with CPU expert offloading
#
# ~20 t/s at 64K context. 116.8B total params, ~5.1B active per token.
# Layers 0-11 on RTX 4090, layers 12-15 on RTX 5070 Ti, remaining on CPU.
# VRAM: 4090 at 96%, 5070 Ti at 89%.
#
# Memory breakdown:
#   Model weights on GPU: ~29 GB (across both GPUs)
#   Model weights on CPU: ~61 GB (mmap)
#   KV cache (64K, q8_0): ~1.3 GB
#   Compute buffers: ~6.2 GB
#
# Reasoning levels: GPT-OSS uses a system prompt trigger, not a server flag.
# Set "Reasoning: low/medium/high" in the system prompt of your client (web UI,
# API call, etc.). llama-server's --system-prompt flag is excluded from the
# server binary, so this CANNOT be set via EXTRA_ARGS or server config.
#
# See docs/gpt-oss-120b-configuration-guide.md for full details.
# -----------------------------------------------------------------------------

[gpt-oss-120b]
NAME=GPT-OSS 120B F16
MODEL=GPT-OSS-120b/gpt-oss-120b-F16.gguf
CTX_SIZE=65536
N_GPU_LAYERS=99
FIT=off
EXTRA_ARGS=--jinja -np 1 --temp 1.0 --top-p 1.0 -b 4096 -ub 4096 -ot blk\.([0-9]|1[01])\.=CUDA0,blk\.(1[2-5])\.=CUDA1,exps=CPU


# -----------------------------------------------------------------------------
# Qwen3-Coder-Next — 80B MoE (512 experts, 10 active, ~3B active/token)
#
# 75% DeltaNet (linear attention, no KV cache) + 25% standard attention.
# Only 12/48 layers have KV cache, making 256K context feasible.
#
# Unsloth Dynamic (UD) quants give higher precision to expert router tensors,
# resulting in better expert selection and no self-correction behavior.
#
# See docs/llama-cpp-flags-and-qwen3-strategy.md for full details.
# -----------------------------------------------------------------------------

# Speed option: 25.8 t/s, 15+7 layers on GPU (22/48)
# Strategy 3 from docs — same as UD-Q6_K_XL but +21% faster
[qwen3-coder-q5]
NAME=Qwen3-Coder-Next UD-Q5_K_XL (speed)
MODEL=Qwen3-Coder-Next/UD-Q5_K_XL/Qwen3-Coder-Next-UD-Q5_K_XL-00001-of-00003.gguf
CTX_SIZE=262144
N_GPU_LAYERS=99
FIT=off
EXTRA_ARGS=--jinja -np 1 -b 2048 -ub 2048 --no-context-shift --temp 1.0 --top-p 0.95 --top-k 40 --min-p 0.01 -ot blk\.([0-9]|1[0-4])\.=CUDA0,blk\.(1[5-9]|2[0-1])\.=CUDA1,exps=CPU

# Baseline: 21.4 t/s, 13+6 layers on GPU (19/48), best quality
# Strategy 5 from docs — recommended default
[qwen3-coder]
NAME=Qwen3-Coder-Next UD-Q6_K_XL (baseline)
MODEL=Qwen3-Coder-Next/UD-Q6_K_XL/Qwen3-Coder-Next-UD-Q6_K_XL-00001-of-00003.gguf
CTX_SIZE=262144
N_GPU_LAYERS=99
FIT=off
EXTRA_ARGS=--jinja -np 1 -b 2048 -ub 2048 --no-context-shift --temp 1.0 --top-p 0.95 --top-k 40 --min-p 0.01 -ot blk\.([0-9]|1[0-2])\.=CUDA0,blk\.(1[3-8])\.=CUDA1,exps=CPU

# Standard Q6_K (non-UD): ~21 t/s, 13+6 layers on GPU (19/48)
# Same layer split as UD-Q6_K_XL since model sizes are similar.
# For most use cases, prefer qwen3-coder (UD-Q6_K_XL) for better quality.
[qwen3-coder-q6k]
NAME=Qwen3-Coder-Next Q6_K
MODEL=Qwen3-Coder-Next/Q6_K/Qwen3-Coder-Next-Q6_K-00001-of-00003.gguf
CTX_SIZE=262144
N_GPU_LAYERS=99
FIT=off
EXTRA_ARGS=--jinja -np 1 -b 2048 -ub 2048 --no-context-shift --temp 1.0 --top-p 0.95 --top-k 40 --min-p 0.01 -ot blk\.([0-9]|1[0-2])\.=CUDA0,blk\.(1[3-8])\.=CUDA1,exps=CPU


# =============================================================================
# Benchmark profiles (EvalPlus HumanEval+) — server config only
#
# Optimized for benchmarks: 10K context (HumanEval worst case is ~8.4K tokens),
# small batch sizes (prompts are ~400 tokens max), and more GPU layers than
# production (less KV cache + smaller compute buffers = more room for weights).
#
# Key differences from production profiles:
#   - CTX_SIZE=10240 (vs 128K/256K/64K) — saves GB of KV cache
#   - -b 512 -ub 512 (vs 2048-4096) — HumanEval prompts are tiny, smaller
#     micro-batch reduces compute buffer VRAM by several GB
#   - More layers on GPU — freed VRAM used for model weights = faster inference
#   - No sampler args — evalplus sends temperature=0 via API (--greedy)
#
# Client-side config (system prompts) is in benchmarks/evalplus/bench-client.conf.
# Used by: benchmarks/evalplus/benchmark.sh
#
# IMPORTANT: These splits need OOM testing via start.sh before benchmark runs.
# If a profile OOMs, reduce the CUDA1 layer range by 1.
# =============================================================================


# --- GLM-4.7 Flash benchmarks ---
# MoE model: 30B-A3B (64 experts, 4 active + 1 shared, 47 layers).
# Architecture source: model card (models/documentation/README_modelcard_GLM-4.7-Flash.md)
#
# GPU strategy (per docs/gpu-strategy-guide.md):
#   Q4 (18 GB file, ~17.5 GB on GPU): fits entirely on RTX 4090 at 10K context.
#     → Strategy A: single GPU, --split-mode none --main-gpu 0
#   Q8 (30 GB file, ~30 GB on GPU): exceeds 4090, needs both GPUs.
#     → Strategy C: selective GPU split with -ot, no exps=CPU needed because
#       30 GB fits within combined GPU VRAM (4090 24 GB + 5070 Ti 12.5 GB usable).
#       Maximize layers on CUDA0, overflow to CUDA1.
#
# --reasoning-format none: GLM is a thinking model. Without this flag, the
# chain-of-thought goes into a separate reasoning_content field that evalplus
# ignores, resulting in empty solutions. With this flag, everything (thinking +
# answer) goes into the content field. The postprocessor then strips the
# <think> tags, leaving only the code. Production profiles don't need this
# because the web UI renders reasoning_content natively.

# Q4: all on RTX 4090 (Strategy A). Verified: 17,285 MiB model buffer,
# 18,361 MiB total VRAM = 74% of 4090. No inter-GPU transfers, graph splits = 2
# (CPU↔GPU0 only for tok_embd, which llama.cpp always keeps on CPU).
# Measured: ~140 t/s.
#
# Old broken config (before 2026-02-15): used FIT=on without --split-mode none,
# which caused llama.cpp to unnecessarily split the 18 GB model across both GPUs.
# This added inter-GPU transfers for no benefit — Q4 fits entirely on the 4090.
#   FIT=on
#   EXTRA_ARGS=--jinja -np 1 --reasoning-format none
[bench-glm-flash-q4]
NAME=GLM-4.7 Flash Q4_K_M (benchmark)
MODEL=GLM-4.7-Flash/GLM-4.7-Flash-Q4_K_M.gguf
CTX_SIZE=10240
N_GPU_LAYERS=99
FIT=off
EXTRA_ARGS=--jinja -np 1 --reasoning-format none --split-mode none --main-gpu 0

# Q8: split across both GPUs (Strategy C). 30 GB model needs both GPUs but
# fits within combined VRAM, so no exps=CPU needed — all experts stay on GPU.
# 47 layers at ~628 MiB/layer (measured).
#
# Tested 35+12: 33 graph splits, ~105 t/s. CUDA0: 22.0/24.6 GB (headroom ~1.8 GB).
# Tested 37+10: 53 graph splits, ~102 t/s. More splits = slower despite more on 4090.
# The split boundary affects MoE graph scheduling — fewer splits wins over more
# layers on the faster GPU. See docs/gpu-strategy-guide.md "Graph splits" section.
#
# 35+12 is the sweet spot: fewest splits + most on fastest GPU.
# Tested 37+10 (slower, more splits):
#   EXTRA_ARGS=... -ot blk\.([0-9]|[12][0-9]|3[0-6])\.=CUDA0,blk\.(3[7-9]|4[0-6])\.=CUDA1
[bench-glm-flash-q8]
NAME=GLM-4.7 Flash Q8_0 (benchmark)
MODEL=GLM-4.7-Flash/GLM-4.7-Flash-Q8_0.gguf
CTX_SIZE=10240
N_GPU_LAYERS=99
FIT=off
EXTRA_ARGS=--jinja -np 1 --reasoning-format none -b 512 -ub 512 -ot blk\.([0-9]|[12][0-9]|3[0-4])\.=CUDA0,blk\.(3[5-9]|4[0-6])\.=CUDA1


# --- GPT-OSS 120B benchmark ---
# MoE, 36 layers (~1.7 GB/layer). Production: 12 CUDA0 + 4 CUDA1 at 64K/-ub 4096.
# At 10K with -ub 512: KV cache drops from ~1.3 GB to ~0.2 GB, compute buffer
# shrinks by ~1.3 GB on CUDA0 and ~2.5 GB on CUDA1. Total freed: ~2.7 GB CUDA0,
# ~2.5 GB CUDA1 — enough for +1 layer on each GPU vs production.
# Now 13 CUDA0 + 5 CUDA1 = 18/36 (was 12+5=17 before, 12+4=16 production).
#
# Previous bench split (before 2026-02-15): 12 CUDA0 + 5 CUDA1 = 17/36.
# CUDA0 had ~2.7 GB headroom — room for 1 more layer.
#   EXTRA_ARGS=... -ot blk\.([0-9]|1[01])\.=CUDA0,blk\.(1[2-6])\.=CUDA1,exps=CPU
#
# UNTESTED — if OOM on CUDA0, revert to 12+5 (change 1[0-2] back to 1[01]).

[bench-gpt-oss-120b]
NAME=GPT-OSS 120B F16 (benchmark)
MODEL=GPT-OSS-120b/gpt-oss-120b-F16.gguf
CTX_SIZE=10240
N_GPU_LAYERS=99
FIT=off
EXTRA_ARGS=--jinja -np 1 -b 512 -ub 512 -ot blk\.([0-9]|1[0-2])\.=CUDA0,blk\.(1[3-7])\.=CUDA1,exps=CPU


# --- Qwen3-Coder-Next benchmarks ---
# MoE with DeltaNet: only 12/48 layers have KV cache. Production at 256K:
# KV ~3.2 GB. At 10K: ~128 MB (saves ~3 GB!). Combined with -ub 512 (saves
# compute buffer), total ~4-5 GB freed per GPU. Each layer ~1.3 GB (Q6).
#
# Q5 production: 15 CUDA0 + 7 CUDA1 = 22/48. Bench: 18 CUDA0 + 8 CUDA1 = 26/48.
# Q6 production: 13 CUDA0 + 6 CUDA1 = 19/48. Bench: 16 CUDA0 + 7 CUDA1 = 23/48.

[bench-qwen3-coder-ud-q5]
NAME=Qwen3-Coder-Next UD-Q5_K_XL (benchmark)
MODEL=Qwen3-Coder-Next/UD-Q5_K_XL/Qwen3-Coder-Next-UD-Q5_K_XL-00001-of-00003.gguf
CTX_SIZE=10240
N_GPU_LAYERS=99
FIT=off
EXTRA_ARGS=--jinja -np 1 -b 512 -ub 512 --no-context-shift -ot blk\.([0-9]|1[0-7])\.=CUDA0,blk\.(1[8-9]|2[0-5])\.=CUDA1,exps=CPU

[bench-qwen3-coder-ud-q6]
NAME=Qwen3-Coder-Next UD-Q6_K_XL (benchmark)
MODEL=Qwen3-Coder-Next/UD-Q6_K_XL/Qwen3-Coder-Next-UD-Q6_K_XL-00001-of-00003.gguf
CTX_SIZE=10240
N_GPU_LAYERS=99
FIT=off
EXTRA_ARGS=--jinja -np 1 -b 512 -ub 512 --no-context-shift -ot blk\.([0-9]|1[0-5])\.=CUDA0,blk\.(1[6-9]|2[0-2])\.=CUDA1,exps=CPU

[bench-qwen3-coder-q6k]
NAME=Qwen3-Coder-Next Q6_K (benchmark)
MODEL=Qwen3-Coder-Next/Q6_K/Qwen3-Coder-Next-Q6_K-00001-of-00003.gguf
CTX_SIZE=10240
N_GPU_LAYERS=99
FIT=off
EXTRA_ARGS=--jinja -np 1 -b 512 -ub 512 --no-context-shift -ot blk\.([0-9]|1[0-5])\.=CUDA0,blk\.(1[6-9]|2[0-2])\.=CUDA1,exps=CPU

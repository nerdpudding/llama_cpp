# GPT-OSS 120B F16 (MXFP4 native) â€” MoE with CPU expert offloading
#
# ~20 t/s at 64K context. 116.8B total params, ~5.1B active per token.
# Layers 0-11 on RTX 4090, layers 12-15 on RTX 5070 Ti, remaining experts on CPU.
# VRAM: 4090 at 96%, 5070 Ti at 89%.
#
# Memory breakdown:
#   Model weights on GPU: ~29 GB (across both GPUs)
#   Model weights on CPU: ~61 GB (mmap)
#   KV cache (64K, q8_0): ~1.3 GB
#   Compute buffers: ~6.2 GB
#
# See docs/gpt-oss-120b-configuration-guide.md for full details.

MODEL=GPT-OSS-120b/gpt-oss-120b-F16.gguf
CTX_SIZE=65536
N_GPU_LAYERS=99
FIT=off
EXTRA_ARGS=--jinja -np 1 -b 4096 -ub 4096 -ot blk\.([0-9]|1[01])\.=CUDA0,blk\.(1[2-5])\.=CUDA1,exps=CPU

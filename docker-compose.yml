# =============================================================================
# llama.cpp server -- RTX 4090 + RTX 5070 Ti
#
# Quick start:
#   MODEL=my-model.gguf docker compose up
#
# Override any variable below on the command line, in a .env file, or by
# editing the environment section directly.
# =============================================================================

services:
  llama-server:
    build: .
    container_name: llama-server
    ports:
      - "8080:8080"
    volumes:
      - ./models:/models:ro
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    environment:
      # ---- Model ----
      - MODEL=/models/${MODEL:-model.gguf}
      # ---- Context ----
      - CTX_SIZE=${CTX_SIZE:-131072}
      # ---- GPU offload ----
      - N_GPU_LAYERS=${N_GPU_LAYERS:-99}
      # ---- Split mode: layer (default), row, none ----
      - SPLIT_MODE=${SPLIT_MODE:-layer}
      # ---- Manual tensor split ratio (blank = use --fit auto) ----
      - TENSOR_SPLIT=${TENSOR_SPLIT:-}
      # ---- Main GPU (0 = RTX 4090, 1 = RTX 5070 Ti) ----
      - MAIN_GPU=${MAIN_GPU:-0}
      # ---- Flash attention ----
      - FLASH_ATTN=${FLASH_ATTN:-1}
      # ---- KV cache quantization ----
      - KV_CACHE_TYPE_K=${KV_CACHE_TYPE_K:-q8_0}
      - KV_CACHE_TYPE_V=${KV_CACHE_TYPE_V:-q8_0}
      # ---- Auto-fit (on by default, adjusts layers/ctx to fit VRAM) ----
      - FIT=${FIT:-on}
      # ---- Fit target: VRAM headroom in MiB per device (e.g. 1024 or 512,4096) ----
      - FIT_TARGET=${FIT_TARGET:-}
      # ---- Fit minimum context size ----
      - FIT_CTX=${FIT_CTX:-}
      # ---- Extra flags (pass anything not covered above) ----
      - EXTRA_ARGS=${EXTRA_ARGS:-}
    restart: unless-stopped
